{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6b6ebc",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-09T18:46:45.507962Z",
     "iopub.status.busy": "2025-04-09T18:46:45.507605Z",
     "iopub.status.idle": "2025-04-10T02:11:50.396984Z",
     "shell.execute_reply": "2025-04-10T02:11:50.395653Z"
    },
    "papermill": {
     "duration": 26704.895406,
     "end_time": "2025-04-10T02:11:50.398749",
     "exception": false,
     "start_time": "2025-04-09T18:46:45.503343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with manhattan and contrastive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 20/4436 [01:30<4:49:10,  3.93s/it, loss=0.687]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debug checkpoint reached - continuing training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 4436/4436 [5:14:10<00:00,  4.25s/it, loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 0.2490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 1731/1731 [19:42<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@5: 0.0682\n",
      "Model saved with mAP@5: 0.0682\n",
      "Evaluating best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 1731/1731 [20:44<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@5: 0.0682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 1731/1731 [20:44<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# import wandb\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    train_csv = \"/kaggle/input/visual-product-recognition/train.csv\"\n",
    "    test_csv = \"/kaggle/input/visual-product-recognition/test.csv\"\n",
    "    train_dir = \"/kaggle/input/visual-product-recognition/train/train\"\n",
    "    test_dir = \"/kaggle/input/visual-product-recognition/test/test\"\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size = 32\n",
    "    num_epochs = 1\n",
    "    learning_rate = 0.001\n",
    "    embedding_dim = 128\n",
    "    \n",
    "    # Model parameters\n",
    "    distance_metric = \"manhattan\"  # Options: \"cosine\", \"euclidean\", \"manhattan\"\n",
    "    loss_type = \"contrastive\"   # Options: \"contrastive\", \"triplet\"\n",
    "    margin = 1.0  # Margin for contrastive/triplet loss\n",
    "    \n",
    "    # Image parameters\n",
    "    img_size = 224\n",
    "    \n",
    "    # Checkpointing\n",
    "    checkpoint_path = f\"siamese_model_{distance_metric}_{loss_type}.pth\"\n",
    "    \n",
    "    # Evaluation\n",
    "    top_k = 5  # For mAP@k calculation\n",
    "\n",
    "# Initialize wandb\n",
    "# wandb.init(project=\"product-image-retrieval\", config=vars(Config))\n",
    "\n",
    "# Custom Dataset\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None, is_train=True):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Combine class and group to create a unique label\n",
    "        if 'group' in self.data.columns:\n",
    "            self.data['label'] = self.data['class'].astype(str) + '_' + self.data['group'].astype(str)\n",
    "        else:\n",
    "            self.data['label'] = self.data['class'].astype(str)\n",
    "            \n",
    "        self.labels = self.data['label'].unique()\n",
    "        self.label_to_indices = {label: np.where(self.data['label'] == label)[0] for label in self.labels}\n",
    "        \n",
    "        # For triplet loss: create a list of (anchor, positive, negative) indices\n",
    "        if Config.loss_type == \"triplet\" and is_train:\n",
    "            self.triplets = self._generate_triplets()\n",
    "    \n",
    "    def _generate_triplets(self):\n",
    "        triplets = []\n",
    "        for label in self.labels:\n",
    "            label_indices = self.label_to_indices[label]\n",
    "            if len(label_indices) < 2:\n",
    "                continue\n",
    "                \n",
    "            for i in range(len(label_indices)):\n",
    "                anchor_idx = label_indices[i]\n",
    "                positive_indices = [idx for idx in label_indices if idx != anchor_idx]\n",
    "                \n",
    "                if not positive_indices:\n",
    "                    continue\n",
    "                    \n",
    "                positive_idx = random.choice(positive_indices)\n",
    "                \n",
    "                # Select a negative from a different label\n",
    "                negative_label = random.choice([l for l in self.labels if l != label])\n",
    "                negative_idx = random.choice(self.label_to_indices[negative_label])\n",
    "                \n",
    "                triplets.append((anchor_idx, positive_idx, negative_idx))\n",
    "        \n",
    "        return triplets\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.is_train and Config.loss_type == \"triplet\":\n",
    "            return len(self.triplets)\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train and Config.loss_type == \"triplet\":\n",
    "            anchor_idx, positive_idx, negative_idx = self.triplets[idx]\n",
    "            \n",
    "            anchor_img = self._load_image(anchor_idx)\n",
    "            positive_img = self._load_image(positive_idx)\n",
    "            negative_img = self._load_image(negative_idx)\n",
    "            \n",
    "            anchor_label = self.data.iloc[anchor_idx]['label']\n",
    "            positive_label = self.data.iloc[positive_idx]['label']\n",
    "            negative_label = self.data.iloc[negative_idx]['label']\n",
    "            \n",
    "            return (anchor_img, positive_img, negative_img), (anchor_label, positive_label, negative_label)\n",
    "        else:\n",
    "            img = self._load_image(idx)\n",
    "            label = self.data.iloc[idx]['label']\n",
    "            img_name = self.data.iloc[idx]['name']\n",
    "            \n",
    "            if Config.loss_type == \"contrastive\" and self.is_train:\n",
    "                # 50% chance to get a positive pair (same label)\n",
    "                if random.random() > 0.5:\n",
    "                    indices = self.label_to_indices[label]\n",
    "                    idx2 = random.choice([i for i in indices if i != idx]) if len(indices) > 1 else idx\n",
    "                    is_same = 1\n",
    "                else:\n",
    "                    # Get a sample from a different label\n",
    "                    different_label = random.choice([l for l in self.labels if l != label])\n",
    "                    idx2 = random.choice(self.label_to_indices[different_label])\n",
    "                    is_same = 0\n",
    "                \n",
    "                img2 = self._load_image(idx2)\n",
    "                label2 = self.data.iloc[idx2]['label']\n",
    "                \n",
    "                return (img, img2), (label, label2, is_same)\n",
    "            else:\n",
    "                # For evaluation or when not using contrastive loss\n",
    "                return img, label, img_name\n",
    "    \n",
    "    def _load_image(self, idx):\n",
    "        img_name = self.data.iloc[idx]['name']\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n",
    "\n",
    "# Define the CNN encoder (Siamese base network)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # CNN layers\n",
    "        self.features = nn.Sequential(\n",
    "            # First convolutional block\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Second convolutional block\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Third convolutional block\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Fourth convolutional block\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Fifth convolutional block\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        # L2 normalize embeddings\n",
    "        x = nn.functional.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "# Siamese Network\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.encoder = Encoder(embedding_dim)\n",
    "    \n",
    "    def forward_one(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def forward(self, x1, x2=None, x3=None):\n",
    "        output1 = self.forward_one(x1)\n",
    "        \n",
    "        if x2 is not None and x3 is not None:  # Triplet input\n",
    "            output2 = self.forward_one(x2)\n",
    "            output3 = self.forward_one(x3)\n",
    "            return output1, output2, output3\n",
    "        elif x2 is not None:  # Pair input\n",
    "            output2 = self.forward_one(x2)\n",
    "            return output1, output2\n",
    "        else:  # Single input (for inference)\n",
    "            return output1\n",
    "\n",
    "# Loss Functions\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0, distance_metric=\"cosine\"):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.distance_metric = distance_metric\n",
    "        \n",
    "    def forward(self, output1, output2, target):\n",
    "        if self.distance_metric == \"cosine\":\n",
    "            # Using negative cosine similarity (1 - cos) to convert similarity to distance\n",
    "            distance = 1.0 - nn.functional.cosine_similarity(output1, output2)\n",
    "        elif self.distance_metric == \"euclidean\":\n",
    "            distance = nn.functional.pairwise_distance(output1, output2, p=2)\n",
    "        elif self.distance_metric == \"manhattan\":\n",
    "            distance = nn.functional.pairwise_distance(output1, output2, p=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported distance metric: {self.distance_metric}\")\n",
    "        \n",
    "        # Contrastive loss: bring positives together, push negatives apart beyond margin\n",
    "        loss = target * torch.pow(distance, 2) + (1 - target) * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2)\n",
    "        return loss.mean()\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0, distance_metric=\"cosine\"):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.distance_metric = distance_metric\n",
    "        \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        if self.distance_metric == \"cosine\":\n",
    "            # Convert cosine similarity to distance\n",
    "            pos_dist = 1.0 - nn.functional.cosine_similarity(anchor, positive)\n",
    "            neg_dist = 1.0 - nn.functional.cosine_similarity(anchor, negative)\n",
    "        elif self.distance_metric == \"euclidean\":\n",
    "            pos_dist = nn.functional.pairwise_distance(anchor, positive, p=2)\n",
    "            neg_dist = nn.functional.pairwise_distance(anchor, negative, p=2)\n",
    "        elif self.distance_metric == \"manhattan\":\n",
    "            pos_dist = nn.functional.pairwise_distance(anchor, positive, p=1)\n",
    "            neg_dist = nn.functional.pairwise_distance(anchor, negative, p=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported distance metric: {self.distance_metric}\")\n",
    "            \n",
    "        # Triplet loss: ensure positive distance is smaller than negative distance by at least margin\n",
    "        loss = torch.clamp(pos_dist - neg_dist + self.margin, min=0.0)\n",
    "        return loss.mean()\n",
    "\n",
    "# Data Transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((Config.img_size, Config.img_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((Config.img_size, Config.img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ProductDataset(\n",
    "    csv_file=Config.train_csv,\n",
    "    img_dir=Config.train_dir,\n",
    "    transform=data_transforms['train'],\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "test_dataset = ProductDataset(\n",
    "    csv_file=Config.test_csv,\n",
    "    img_dir=Config.test_dir,\n",
    "    transform=data_transforms['test'],\n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = SiameseNetwork(embedding_dim=Config.embedding_dim)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\n",
    "\n",
    "# Initialize loss function\n",
    "if Config.loss_type == \"contrastive\":\n",
    "    criterion = ContrastiveLoss(margin=Config.margin, distance_metric=Config.distance_metric)\n",
    "elif Config.loss_type == \"triplet\":\n",
    "    criterion = TripletLoss(margin=Config.margin, distance_metric=Config.distance_metric)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported loss type: {Config.loss_type}\")\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with tqdm(dataloader, desc=f\"Epoch {epoch+1}/{Config.num_epochs}\") as pbar:\n",
    "        for i, data in enumerate(pbar):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if Config.loss_type == \"triplet\":\n",
    "                (anchor_img, positive_img, negative_img), _ = data\n",
    "                anchor_emb, positive_emb, negative_emb = model(anchor_img, positive_img, negative_img)\n",
    "                loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "            else:  # Contrastive loss\n",
    "                (img1, img2), (_, _, labels) = data\n",
    "                output1, output2 = model(img1, img2)\n",
    "                loss = criterion(output1, output2, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix(loss=running_loss/(i+1))\n",
    "            \n",
    "            # Debug pause after a few iterations (comment out in actual training)\n",
    "            if i == 20 and epoch == 0:\n",
    "                print(\"\\nDebug checkpoint reached - continuing training...\\n\")\n",
    "                time.sleep(2)  # Pause for 2 seconds\n",
    "            \n",
    "            # Log to wandb periodically\n",
    "            if i % 10 == 0:\n",
    "                # wandb.log({\"train_loss\": loss.item(), \"epoch\": epoch + i/len(dataloader)})\n",
    "                ...\n",
    "    \n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "# Function to extract embeddings\n",
    "def extract_embeddings(dataloader, model):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    img_names = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, batch_labels, batch_img_names in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "            outputs = model.forward_one(images)\n",
    "            embeddings.append(outputs)\n",
    "            labels.extend(batch_labels)\n",
    "            img_names.extend(batch_img_names)\n",
    "    \n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    return embeddings, labels, img_names\n",
    "\n",
    "# Function to calculate distances between query and gallery embeddings\n",
    "def calculate_distances(query_emb, gallery_emb, distance_metric=\"cosine\"):\n",
    "    if distance_metric == \"cosine\":\n",
    "        # Convert cosine similarity to distance (1 - similarity)\n",
    "        # Higher similarity = lower distance\n",
    "        similarity = torch.mm(query_emb, gallery_emb.T)\n",
    "        return 1.0 - similarity\n",
    "    elif distance_metric == \"euclidean\":\n",
    "        n_query = query_emb.size(0)\n",
    "        n_gallery = gallery_emb.size(0)\n",
    "        dist = torch.zeros(n_query, n_gallery)\n",
    "        \n",
    "        for i in range(n_query):\n",
    "            dist[i] = torch.sum((gallery_emb - query_emb[i].unsqueeze(0))**2, dim=1).sqrt()\n",
    "        return dist\n",
    "    elif distance_metric == \"manhattan\":\n",
    "        n_query = query_emb.size(0)\n",
    "        n_gallery = gallery_emb.size(0)\n",
    "        dist = torch.zeros(n_query, n_gallery)\n",
    "        \n",
    "        for i in range(n_query):\n",
    "            dist[i] = torch.sum(torch.abs(gallery_emb - query_emb[i].unsqueeze(0)), dim=1)\n",
    "        return dist\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distance metric: {distance_metric}\")\n",
    "\n",
    "# Function to calculate mAP@k\n",
    "def calculate_map_at_k(distances, query_labels, gallery_labels, k=5):\n",
    "    \"\"\"\n",
    "    Calculate mean Average Precision at k\n",
    "    \"\"\"\n",
    "    n_query = distances.size(0)\n",
    "    ap_list = []\n",
    "    \n",
    "    for i in range(n_query):\n",
    "        # Get indices of top-k nearest neighbors \n",
    "        _, indices = torch.topk(distances[i], k=min(k, len(gallery_labels)), largest=False)\n",
    "        \n",
    "        # Check if the retrieved items have the same label\n",
    "        relevant = [gallery_labels[idx] == query_labels[i] for idx in indices]\n",
    "        \n",
    "        if not any(relevant):\n",
    "            ap_list.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # Calculate precision at each relevant position\n",
    "        precision_at_i = 0.0\n",
    "        num_relevant = 0.0\n",
    "        \n",
    "        for j, is_relevant in enumerate(relevant):\n",
    "            if is_relevant:\n",
    "                num_relevant += 1\n",
    "                precision_at_i += num_relevant / (j + 1)\n",
    "        \n",
    "        ap = precision_at_i / num_relevant\n",
    "        ap_list.append(ap)\n",
    "    \n",
    "    return np.mean(ap_list)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate(model, test_loader, distance_metric=\"cosine\", k=5):\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract gallery embeddings (using test set as both query and gallery for simplicity)\n",
    "    gallery_embeddings, gallery_labels, gallery_img_names = extract_embeddings(test_loader, model)\n",
    "    \n",
    "    # Calculate distances\n",
    "    distances = calculate_distances(gallery_embeddings, gallery_embeddings, distance_metric)\n",
    "    \n",
    "    # Set diagonal to infinity to exclude self-matches\n",
    "    distances.fill_diagonal_(float('inf'))\n",
    "    \n",
    "    # Calculate mAP@k\n",
    "    map_at_k = calculate_map_at_k(distances, gallery_labels, gallery_labels, k)\n",
    "    \n",
    "    print(f\"mAP@{k}: {map_at_k:.4f}\")\n",
    "    # wandb.log({\"mAP@k\": map_at_k})\n",
    "    \n",
    "    return map_at_k\n",
    "\n",
    "# Image retrieval function\n",
    "def retrieve_images(model, query_img_path, gallery_loader, top_k=5, distance_metric=\"cosine\"):\n",
    "    \"\"\"\n",
    "    Retrieve top-k similar images for a query image\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess query image\n",
    "    query_img = Image.open(query_img_path).convert('RGB')\n",
    "    query_img = data_transforms['test'](query_img).unsqueeze(0)\n",
    "    \n",
    "    # Extract gallery embeddings\n",
    "    gallery_embeddings, _, gallery_img_names = extract_embeddings(gallery_loader, model)\n",
    "    \n",
    "    # Extract query embedding\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model.forward_one(query_img)\n",
    "    \n",
    "    # Calculate distances\n",
    "    if distance_metric == \"cosine\":\n",
    "        similarity = torch.mm(query_embedding, gallery_embeddings.T).squeeze()\n",
    "        distances = 1.0 - similarity\n",
    "    elif distance_metric == \"euclidean\":\n",
    "        distances = torch.sum((gallery_embeddings - query_embedding)**2, dim=1).sqrt()\n",
    "    elif distance_metric == \"manhattan\":\n",
    "        distances = torch.sum(torch.abs(gallery_embeddings - query_embedding), dim=1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distance metric: {distance_metric}\")\n",
    "    \n",
    "    # Get indices of top-k nearest neighbors\n",
    "    _, indices = torch.topk(distances, k=min(top_k, len(gallery_img_names)), largest=False)\n",
    "    \n",
    "    # Return top-k image names and distances\n",
    "    results = [(gallery_img_names[idx], distances[idx].item()) for idx in indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to visualize retrieval results\n",
    "def visualize_retrieval(query_img_path, results, img_dir):\n",
    "    \"\"\"\n",
    "    Visualize query image and retrieved results\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    \n",
    "    # Display query image\n",
    "    query_img = Image.open(query_img_path).convert('RGB')\n",
    "    plt.subplot(1, len(results)+1, 1)\n",
    "    plt.imshow(query_img)\n",
    "    plt.title(\"Query Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Display retrieved images\n",
    "    for i, (img_name, distance) in enumerate(results):\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        plt.subplot(1, len(results)+1, i+2)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Dist: {distance:.4f}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"retrieval_results.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Log to wandb\n",
    "    # wandb.log({\"retrieval_results\": wandb.Image(\"retrieval_results.png\")})\n",
    "\n",
    "# Main training loop\n",
    "def train_model():\n",
    "    best_map = 0.0\n",
    "    \n",
    "    for epoch in range(Config.num_epochs):\n",
    "        epoch_loss = train_epoch(model, train_loader, optimizer, criterion, epoch)\n",
    "        print(f\"Epoch {epoch+1}/{Config.num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate every 5 epochs or on the last epoch\n",
    "        if (epoch + 1) % 5 == 0 or epoch == Config.num_epochs - 1:\n",
    "            map_at_k = evaluate(model, test_loader, Config.distance_metric, Config.top_k)\n",
    "            \n",
    "            # Save model if better\n",
    "            if map_at_k > best_map:\n",
    "                best_map = map_at_k\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': epoch_loss,\n",
    "                    'map': map_at_k\n",
    "                }, Config.checkpoint_path)\n",
    "                print(f\"Model saved with mAP@{Config.top_k}: {map_at_k:.4f}\")\n",
    "        \n",
    "        # Log to wandb\n",
    "        # wandb.log({\n",
    "        #     \"epoch\": epoch + 1,\n",
    "        #     \"train_loss\": epoch_loss,\n",
    "        # })\n",
    "\n",
    "# Run the training\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting training with {Config.distance_metric} and {Config.loss_type}...\")\n",
    "    train_model()\n",
    "    \n",
    "    # Load best model for evaluation\n",
    "    checkpoint = torch.load(Config.checkpoint_path, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(\"Evaluating best model...\")\n",
    "    evaluate(model, test_loader, Config.distance_metric, Config.top_k)\n",
    "    \n",
    "    # Example of image retrieval - replace with actual image path\n",
    "    query_img_path = os.path.join(Config.test_dir, test_dataset.data.iloc[0]['name'])\n",
    "    results = retrieve_images(model, query_img_path, test_loader, Config.top_k, Config.distance_metric)\n",
    "    visualize_retrieval(query_img_path, results, Config.test_dir)\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    # wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3114765,
     "sourceId": 5368434,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26711.542497,
   "end_time": "2025-04-10T02:11:54.313425",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-09T18:46:42.770928",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
