{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d37b4e6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-22T22:25:25.529782Z",
     "iopub.status.busy": "2025-04-22T22:25:25.529390Z",
     "iopub.status.idle": "2025-04-23T06:29:51.562401Z",
     "shell.execute_reply": "2025-04-23T06:29:51.561449Z"
    },
    "papermill": {
     "duration": 29066.037938,
     "end_time": "2025-04-23T06:29:51.563777",
     "exception": false,
     "start_time": "2025-04-22T22:25:25.525839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using backbone: resnet101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-cd907fc2.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-cd907fc2.pth\n",
      "100%|██████████| 171M/171M [00:00<00:00, 202MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading backbone\n",
      "Model initialized successfully.\n",
      "Starting training with euclidean, contrastive, and resnet101...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 20/4436 [00:15<39:59,  1.84it/s, loss=0.262]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug checkpoint reached - continuing training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 4436/4436 [45:51<00:00,  1.61it/s, loss=0.197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 4436/4436 [45:14<00:00,  1.63it/s, loss=0.167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.1666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 4436/4436 [45:25<00:00,  1.63it/s, loss=0.157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.1574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 4436/4436 [45:34<00:00,  1.62it/s, loss=0.152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.1516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 4436/4436 [45:48<00:00,  1.61it/s, loss=0.147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.1466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 1731/1731 [05:27<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@5: 0.0971\n",
      "Model saved with mAP@5: 0.0971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 4436/4436 [46:30<00:00,  1.59it/s, loss=0.142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.1420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 4436/4436 [46:04<00:00,  1.60it/s, loss=0.138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.1382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 4436/4436 [46:30<00:00,  1.59it/s, loss=0.135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.1351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 4436/4436 [46:16<00:00,  1.60it/s, loss=0.132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.1317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 4436/4436 [45:58<00:00,  1.61it/s, loss=0.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.1302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 1731/1731 [05:18<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@5: 0.1225\n",
      "Model saved with mAP@5: 0.1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 1731/1731 [05:03<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@5: 0.1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 1731/1731 [04:54<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import timm  # For EfficientNet\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    train_csv = \"/kaggle/input/visual-product-recognition/train.csv\"\n",
    "    test_csv = \"/kaggle/input/visual-product-recognition/test.csv\"\n",
    "    train_dir = \"/kaggle/input/visual-product-recognition/train/train\"\n",
    "    test_dir = \"/kaggle/input/visual-product-recognition/test/test\"\n",
    "    # Training parameters\n",
    "    batch_size = 32\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "    embedding_dim = 128\n",
    "    # Model parameters\n",
    "    distance_metric = \"euclidean\"  # Options: \"cosine\", \"euclidean\", \"manhattan\"\n",
    "    loss_type = \"contrastive\"   # Options: \"contrastive\", \"triplet\", \"angular\"\n",
    "    margin = 1.0  # Margin for contrastive/triplet/angular loss\n",
    "    backbone = \"resnet101\"  # Options: \"resnet18\", \"efficientnet_b0\"\n",
    "    # Image parameters\n",
    "    img_size = 224\n",
    "    # Checkpointing\n",
    "    checkpoint_path = f\"siamese_model_{distance_metric}_{loss_type}.pth\"\n",
    "    # Evaluation\n",
    "    top_k = 5  # For mAP@k calculation\n",
    "\n",
    "# Custom Dataset (unchanged)\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None, is_train=True):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "\n",
    "        # Combine class and group to create a unique label\n",
    "        if 'group' in self.data.columns:\n",
    "            self.data['label'] = self.data['class'].astype(\n",
    "                str) + '_' + self.data['group'].astype(str)\n",
    "        else:\n",
    "            self.data['label'] = self.data['class'].astype(str)\n",
    "\n",
    "        self.labels = self.data['label'].unique()\n",
    "        self.label_to_indices = {label: np.where(self.data['label'] == label)[\n",
    "            0] for label in self.labels}\n",
    "\n",
    "        # For triplet loss: create a list of (anchor, positive, negative) indices\n",
    "        if Config.loss_type == \"triplet\" and is_train:\n",
    "            self.triplets = self._generate_triplets()\n",
    "\n",
    "    def _generate_triplets(self):\n",
    "        triplets = []\n",
    "        for label in self.labels:\n",
    "            label_indices = self.label_to_indices[label]\n",
    "            if len(label_indices) < 2:\n",
    "                continue\n",
    "\n",
    "            for i in range(len(label_indices)):\n",
    "                anchor_idx = label_indices[i]\n",
    "                positive_indices = [\n",
    "                    idx for idx in label_indices if idx != anchor_idx]\n",
    "\n",
    "                if not positive_indices:\n",
    "                    continue\n",
    "\n",
    "                positive_idx = random.choice(positive_indices)\n",
    "\n",
    "                # Select a negative from a different label\n",
    "                negative_label = random.choice(\n",
    "                    [l for l in self.labels if l != label])\n",
    "                negative_idx = random.choice(\n",
    "                    self.label_to_indices[negative_label])\n",
    "\n",
    "                triplets.append((anchor_idx, positive_idx, negative_idx))\n",
    "\n",
    "        return triplets\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.is_train and Config.loss_type == \"triplet\":\n",
    "            return len(self.triplets)\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train and Config.loss_type == \"triplet\":\n",
    "            anchor_idx, positive_idx, negative_idx = self.triplets[idx]\n",
    "\n",
    "            anchor_img = self._load_image(anchor_idx)\n",
    "            positive_img = self._load_image(positive_idx)\n",
    "            negative_img = self._load_image(negative_idx)\n",
    "\n",
    "            anchor_label = self.data.iloc[anchor_idx]['label']\n",
    "            positive_label = self.data.iloc[positive_idx]['label']\n",
    "            negative_label = self.data.iloc[negative_idx]['label']\n",
    "\n",
    "            return (anchor_img, positive_img, negative_img), (anchor_label, positive_label, negative_label)\n",
    "        else:\n",
    "            img = self._load_image(idx)\n",
    "            label = self.data.iloc[idx]['label']\n",
    "            img_name = self.data.iloc[idx]['name']\n",
    "\n",
    "            if Config.loss_type == \"contrastive\" and self.is_train:\n",
    "                # 50% chance to get a positive pair (same label)\n",
    "                if random.random() > 0.5:\n",
    "                    indices = self.label_to_indices[label]\n",
    "                    idx2 = random.choice([i for i in indices if i != idx]) if len(\n",
    "                        indices) > 1 else idx\n",
    "                    is_same = 1\n",
    "                else:\n",
    "                    # Get a sample from a different label\n",
    "                    different_label = random.choice(\n",
    "                        [l for l in self.labels if l != label])\n",
    "                    idx2 = random.choice(\n",
    "                        self.label_to_indices[different_label])\n",
    "                    is_same = 0\n",
    "\n",
    "                img2 = self._load_image(idx2)\n",
    "                label2 = self.data.iloc[idx2]['label']\n",
    "\n",
    "                return (img, img2), (label, label2, is_same)\n",
    "            else:\n",
    "                # For evaluation or when not using contrastive loss\n",
    "                return img, label, img_name\n",
    "\n",
    "    def _load_image(self, idx):\n",
    "        img_name = self.data.iloc[idx]['name']\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "# Define the CNN encoder with backbone options\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, backbone=\"resnet50\"):\n",
    "        super(Encoder, self).__init__()\n",
    "        print(f\"Using backbone: {backbone}\")\n",
    "        if backbone.startswith(\"resnet\"):\n",
    "            if backbone == \"resnet50\":\n",
    "                base_model = models.resnet50(progress=True, weights='DEFAULT')\n",
    "            elif backbone == \"resnet18\":\n",
    "                base_model = models.resnet18(progress=True, weights='DEFAULT')\n",
    "            elif backbone == \"resnet34\":\n",
    "                base_model = models.resnet34(progress=True, weights='DEFAULT')\n",
    "            elif backbone == \"resnet101\":\n",
    "                base_model = models.resnet101(progress=True, weights='DEFAULT')\n",
    "            elif backbone == \"resnet152\":\n",
    "                base_model = models.resnet152(progress=True, weights='DEFAULT')\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ResNet backbone: {backbone}\")\n",
    "            # Remove the final classification layer\n",
    "            modules = list(base_model.children())[:-1]\n",
    "            self.features = nn.Sequential(*modules)\n",
    "            in_features = base_model.fc.in_features\n",
    "        elif backbone.startswith(\"efficientnet\"):\n",
    "            if backbone == \"efficientnet_b0\":\n",
    "                base_model = timm.create_model(\n",
    "                    \"efficientnet_b0\", pretrained=True)\n",
    "            elif backbone == \"efficientnet_b1\":\n",
    "                base_model = timm.create_model(\n",
    "                    \"efficientnet_b1\", pretrained=True)\n",
    "            elif backbone == \"efficientnet_b2\":\n",
    "                base_model = timm.create_model(\n",
    "                    \"efficientnet_b2\", pretrained=True)\n",
    "            elif backbone == \"efficientnet_b3\":\n",
    "                base_model = timm.create_model(\n",
    "                    \"efficientnet_b3\", pretrained=True)\n",
    "            elif backbone == \"efficientnet_b4\":\n",
    "                base_model = timm.create_model(\n",
    "                    \"efficientnet_b4\", pretrained=True)\n",
    "            elif backbone == \"efficientnet_b5\":\n",
    "                base_model = timm.create_model(\n",
    "                    \"efficientnet_b5\", pretrained=True)\n",
    "            elif backbone == \"efficientnet_b6\":\n",
    "                base_model = timm.create_model(\n",
    "                    \"efficientnet_b6\", pretrained=True)\n",
    "            elif backbone == \"efficientnet_b7\":\n",
    "                base_model = timm.create_model(\n",
    "                    \"efficientnet_b7\", pretrained=True)\n",
    "            elif backbone == \"efficientnet_b8\":\n",
    "                base_model = timm.create_model(\n",
    "                    \"efficientnet_b8\", pretrained=True)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unsupported EfficientNet backbone: {backbone}\")\n",
    "            # Remove the final classification layer\n",
    "            self.features = nn.Sequential(*list(base_model.children())[:-1])\n",
    "            in_features = base_model.classifier.in_features\n",
    "\n",
    "        elif backbone.startswith(\"vgg\"):\n",
    "            if backbone == \"vgg16\":\n",
    "                base_model = models.vgg16(progress=True)\n",
    "            elif backbone == \"vgg19\":\n",
    "                base_model = models.vgg19(progress=True)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported VGG backbone: {backbone}\")\n",
    "            # Remove the final classification layer\n",
    "            modules = list(base_model.features.children())\n",
    "            self.features = nn.Sequential(*modules)\n",
    "            in_features = 512 * 7 * 7\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
    "        print(\"Done loading backbone\")\n",
    "        # Fully connected layers for embedding\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        # L2 normalize embeddings\n",
    "        x = nn.functional.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Siamese Network (unchanged)\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, backbone=\"resnet50\"):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.encoder = Encoder(embedding_dim, backbone)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def forward(self, x1, x2=None, x3=None):\n",
    "        output1 = self.forward_one(x1)\n",
    "\n",
    "        if x2 is not None and x3 is not None:  # Triplet input\n",
    "            output2 = self.forward_one(x2)\n",
    "            output3 = self.forward_one(x3)\n",
    "            return output1, output2, output3\n",
    "        elif x2 is not None:  # Pair input\n",
    "            output2 = self.forward_one(x2)\n",
    "            return output1, output2\n",
    "        else:  # Single input (for inference)\n",
    "            return output1\n",
    "\n",
    "# Loss Functions\n",
    "\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0, distance_metric=\"cosine\"):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.distance_metric = distance_metric\n",
    "\n",
    "    def forward(self, output1, output2, target):\n",
    "        if self.distance_metric == \"cosine\":\n",
    "            # Using negative cosine similarity (1 - cos) to convert similarity to distance\n",
    "            distance = 1.0 - nn.functional.cosine_similarity(output1, output2)\n",
    "        elif self.distance_metric == \"euclidean\":\n",
    "            distance = nn.functional.pairwise_distance(output1, output2, p=2)\n",
    "        elif self.distance_metric == \"manhattan\":\n",
    "            distance = nn.functional.pairwise_distance(output1, output2, p=1)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported distance metric: {self.distance_metric}\")\n",
    "\n",
    "        # Contrastive loss: bring positives together, push negatives apart beyond margin\n",
    "        loss = target * torch.pow(distance, 2) + (1 - target) * \\\n",
    "            torch.pow(torch.clamp(self.margin - distance, min=0.0), 2)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0, distance_metric=\"cosine\"):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.distance_metric = distance_metric\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        if self.distance_metric == \"cosine\":\n",
    "            # Convert cosine similarity to distance\n",
    "            pos_dist = 1.0 - nn.functional.cosine_similarity(anchor, positive)\n",
    "            neg_dist = 1.0 - nn.functional.cosine_similarity(anchor, negative)\n",
    "        elif self.distance_metric == \"euclidean\":\n",
    "            pos_dist = nn.functional.pairwise_distance(anchor, positive, p=2)\n",
    "            neg_dist = nn.functional.pairwise_distance(anchor, negative, p=2)\n",
    "        elif self.distance_metric == \"manhattan\":\n",
    "            pos_dist = nn.functional.pairwise_distance(anchor, positive, p=1)\n",
    "            neg_dist = nn.functional.pairwise_distance(anchor, negative, p=1)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported distance metric: {self.distance_metric}\")\n",
    "\n",
    "        # Triplet loss: ensure positive distance is smaller than negative distance by at least margin\n",
    "        loss = torch.clamp(pos_dist - neg_dist + self.margin, min=0.0)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class AngularLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0, alpha=45):\n",
    "        super(AngularLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.alpha = alpha * (np.pi / 180)  # Convert degrees to radians\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # Compute angular distance\n",
    "        ap = nn.functional.pairwise_distance(anchor, positive, p=2)\n",
    "        an = nn.functional.pairwise_distance(anchor, negative, p=2)\n",
    "        angle = torch.atan(torch.sqrt(\n",
    "            (an**2 + ap**2) / (4 * ap**2))) - self.alpha\n",
    "        loss = torch.clamp(ap**2 - 4 * an**2 * torch.cos(angle)\n",
    "                           ** 2 + self.margin, min=0.0)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# Data Transformations (unchanged)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((Config.img_size, Config.img_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((Config.img_size, Config.img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ProductDataset(\n",
    "    csv_file=Config.train_csv,\n",
    "    img_dir=Config.train_dir,\n",
    "    transform=data_transforms['train'],\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "test_dataset = ProductDataset(\n",
    "    csv_file=Config.test_csv,\n",
    "    img_dir=Config.test_dir,\n",
    "    transform=data_transforms['test'],\n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True  # Keep workers alive for faster loading\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = SiameseNetwork(embedding_dim=Config.embedding_dim, backbone=Config.backbone)\n",
    "model = model.to(device)\n",
    "print(\"Model initialized successfully.\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\n",
    "\n",
    "\n",
    "if Config.loss_type == \"contrastive\":\n",
    "    criterion = ContrastiveLoss(\n",
    "        margin=Config.margin, distance_metric=Config.distance_metric).to(device)\n",
    "elif Config.loss_type == \"triplet\":\n",
    "    criterion = TripletLoss(margin=Config.margin,\n",
    "                            distance_metric=Config.distance_metric).to(device)\n",
    "elif Config.loss_type == \"angular\":\n",
    "    criterion = AngularLoss(margin=Config.margin).to(device)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported loss type: {Config.loss_type}\")\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    with tqdm(dataloader, desc=f\"Epoch {epoch+1}/{Config.num_epochs}\") as pbar:\n",
    "        for i, data in enumerate(pbar):\n",
    "            optimizer.zero_grad()\n",
    "            if Config.loss_type == \"triplet\":\n",
    "                # Triplet loss expects anchor, positive, and negative images\n",
    "                (anchor_img, positive_img, negative_img), _ = data\n",
    "                # Move images to the device\n",
    "                anchor_img = anchor_img.to(device)\n",
    "                positive_img = positive_img.to(device)\n",
    "                negative_img = negative_img.to(device)\n",
    "                # Forward pass\n",
    "                anchor_emb, positive_emb, negative_emb = model(anchor_img, positive_img, negative_img)\n",
    "                loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "            elif Config.loss_type == \"contrastive\":\n",
    "                # Contrastive loss expects pairs of images and labels indicating similarity\n",
    "                (img1, img2), (_, _, labels) = data\n",
    "                # Move images and labels to the device\n",
    "                img1 = img1.to(device)\n",
    "                img2 = img2.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # Forward pass\n",
    "                output1, output2 = model(img1, img2)\n",
    "                loss = criterion(output1, output2, labels)\n",
    "            elif Config.loss_type == \"angular\":\n",
    "                # Angular loss also expects anchor, positive, and negative images\n",
    "                (anchor_img, positive_img, negative_img), _ = data\n",
    "                # Move images to the device\n",
    "                anchor_img = anchor_img.to(device)\n",
    "                positive_img = positive_img.to(device)\n",
    "                negative_img = negative_img.to(device)\n",
    "                # Forward pass\n",
    "                anchor_emb, positive_emb, negative_emb = model(anchor_img, positive_img, negative_img)\n",
    "                loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported loss type: {Config.loss_type}\")\n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Update running loss and progress bar\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix(loss=running_loss / (i + 1))\n",
    "            # Debug checkpoint (optional, for testing purposes)\n",
    "            if i == 20 and epoch == 0:\n",
    "                print(\"Debug checkpoint reached - continuing training...\")\n",
    "                time.sleep(2)  # Pause for 2 seconds\n",
    "            # Log metrics periodically (e.g., to wandb or other logging tools)\n",
    "            if i % 10 == 0:\n",
    "                # wandb.log({\"train_loss\": loss.item(), \"epoch\": epoch + i / len(dataloader)})\n",
    "                pass\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "# Function to extract embeddings\n",
    "def extract_embeddings(dataloader, model):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    img_names = []\n",
    "    with torch.no_grad():\n",
    "        for images, batch_labels, batch_img_names in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "            # Move images to the device\n",
    "            images = images.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model.forward_one(images)\n",
    "            # Keep embeddings on the GPU\n",
    "            embeddings.append(outputs)\n",
    "            labels.extend(batch_labels)\n",
    "            img_names.extend(batch_img_names)\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    return embeddings, labels, img_names\n",
    "# Function to calculate distances between query and gallery embeddings\n",
    "\n",
    "\n",
    "def calculate_distances(query_emb, gallery_emb, distance_metric=\"cosine\"):\n",
    "    if distance_metric == \"cosine\":\n",
    "        # Convert cosine similarity to distance (1 - similarity)\n",
    "        # Higher similarity = lower distance\n",
    "        similarity = torch.mm(query_emb, gallery_emb.T)\n",
    "        return 1.0 - similarity\n",
    "    elif distance_metric == \"euclidean\":\n",
    "        n_query = query_emb.size(0)\n",
    "        n_gallery = gallery_emb.size(0)\n",
    "        dist = torch.zeros(n_query, n_gallery)\n",
    "\n",
    "        for i in range(n_query):\n",
    "            dist[i] = torch.sum(\n",
    "                (gallery_emb - query_emb[i].unsqueeze(0))**2, dim=1).sqrt()\n",
    "        return dist\n",
    "    elif distance_metric == \"manhattan\":\n",
    "        n_query = query_emb.size(0)\n",
    "        n_gallery = gallery_emb.size(0)\n",
    "        dist = torch.zeros(n_query, n_gallery)\n",
    "\n",
    "        for i in range(n_query):\n",
    "            dist[i] = torch.sum(\n",
    "                torch.abs(gallery_emb - query_emb[i].unsqueeze(0)), dim=1)\n",
    "        return dist\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distance metric: {distance_metric}\")\n",
    "\n",
    "# Function to calculate mAP@k\n",
    "\n",
    "\n",
    "def calculate_map_at_k(distances, query_labels, gallery_labels, k=5):\n",
    "    \"\"\"\n",
    "    Calculate mean Average Precision at k\n",
    "    \"\"\"\n",
    "    n_query = distances.size(0)\n",
    "    ap_list = []\n",
    "\n",
    "    for i in range(n_query):\n",
    "        # Get indices of top-k nearest neighbors\n",
    "        _, indices = torch.topk(distances[i], k=min(\n",
    "            k, len(gallery_labels)), largest=False)\n",
    "\n",
    "        # Check if the retrieved items have the same label\n",
    "        relevant = [gallery_labels[idx] == query_labels[i] for idx in indices]\n",
    "\n",
    "        if not any(relevant):\n",
    "            ap_list.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Calculate precision at each relevant position\n",
    "        precision_at_i = 0.0\n",
    "        num_relevant = 0.0\n",
    "\n",
    "        for j, is_relevant in enumerate(relevant):\n",
    "            if is_relevant:\n",
    "                num_relevant += 1\n",
    "                precision_at_i += num_relevant / (j + 1)\n",
    "\n",
    "        ap = precision_at_i / num_relevant\n",
    "        ap_list.append(ap)\n",
    "\n",
    "    return np.mean(ap_list)\n",
    "\n",
    "# Function to evaluate the model\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, distance_metric=\"cosine\", k=5):\n",
    "    model.eval()\n",
    "\n",
    "    # Extract gallery embeddings (using test set as both query and gallery for simplicity)\n",
    "    gallery_embeddings, gallery_labels, gallery_img_names = extract_embeddings(\n",
    "        test_loader, model)\n",
    "\n",
    "    # Calculate distances\n",
    "    distances = calculate_distances(\n",
    "        gallery_embeddings, gallery_embeddings, distance_metric)\n",
    "\n",
    "    # Set diagonal to infinity to exclude self-matches\n",
    "    distances.fill_diagonal_(float('inf'))\n",
    "\n",
    "    # Calculate mAP@k\n",
    "    map_at_k = calculate_map_at_k(distances, gallery_labels, gallery_labels, k)\n",
    "\n",
    "    print(f\"mAP@{k}: {map_at_k:.4f}\")\n",
    "    # wandb.log({\"mAP@k\": map_at_k})\n",
    "\n",
    "    return map_at_k\n",
    "\n",
    "# Image retrieval function\n",
    "\n",
    "\n",
    "def retrieve_images(model, query_img_path, gallery_loader, top_k=5, distance_metric=\"cosine\"):\n",
    "    model.eval()\n",
    "    # Load and preprocess query image\n",
    "    query_img = Image.open(query_img_path).convert('RGB')\n",
    "    query_img = data_transforms['test'](query_img).unsqueeze(0)  # Add batch dimension\n",
    "    # Move query image to the GPU\n",
    "    query_img = query_img.to(device)\n",
    "    # Extract gallery embeddings\n",
    "    gallery_embeddings, _, gallery_img_names = extract_embeddings(gallery_loader, model)\n",
    "    # Extract query embedding\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model.forward_one(query_img)\n",
    "    # Calculate distances\n",
    "    if distance_metric == \"cosine\":\n",
    "        similarity = torch.mm(query_embedding, gallery_embeddings.T).squeeze()\n",
    "        distances = 1.0 - similarity\n",
    "    elif distance_metric == \"euclidean\":\n",
    "        distances = torch.sum((gallery_embeddings - query_embedding)**2, dim=1).sqrt()\n",
    "    elif distance_metric == \"manhattan\":\n",
    "        distances = torch.sum(torch.abs(gallery_embeddings - query_embedding), dim=1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distance metric: {distance_metric}\")\n",
    "    # Get indices of top-k nearest neighbors\n",
    "    _, indices = torch.topk(distances, k=min(top_k, len(gallery_img_names)), largest=False)\n",
    "    # Return top-k image names and distances\n",
    "    results = [(gallery_img_names[idx], distances[idx].item()) for idx in indices]\n",
    "    # Move distances to CPU if needed\n",
    "    distances = distances.cpu()\n",
    "    return results\n",
    "# Function to visualize retrieval results\n",
    "\n",
    "\n",
    "def visualize_retrieval(query_img_path, results, img_dir):\n",
    "    \"\"\"\n",
    "    Visualize query image and retrieved results\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 3))\n",
    "\n",
    "    # Display query image\n",
    "    query_img = Image.open(query_img_path).convert('RGB')\n",
    "    plt.subplot(1, len(results)+1, 1)\n",
    "    plt.imshow(query_img)\n",
    "    plt.title(\"Query Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display retrieved images\n",
    "    for i, (img_name, distance) in enumerate(results):\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        plt.subplot(1, len(results)+1, i+2)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Dist: {distance:.4f}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"retrieval_results.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Log to wandb\n",
    "    # wandb.log({\"retrieval_results\": wandb.Image(\"retrieval_results.png\")})\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "def train_model():\n",
    "    best_map = 0.0\n",
    "\n",
    "    for epoch in range(Config.num_epochs):\n",
    "        epoch_loss = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, epoch)\n",
    "        print(f\"Epoch {epoch+1}/{Config.num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Evaluate every 5 epochs or on the last epoch\n",
    "        if (epoch + 1) % 5 == 0 or epoch == Config.num_epochs - 1:\n",
    "            map_at_k = evaluate(model, test_loader,\n",
    "                                Config.distance_metric, Config.top_k)\n",
    "\n",
    "            # Save model if better\n",
    "            if map_at_k > best_map:\n",
    "                best_map = map_at_k\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': epoch_loss,\n",
    "                    'map': map_at_k\n",
    "                }, Config.checkpoint_path)\n",
    "                print(f\"Model saved with mAP@{Config.top_k}: {map_at_k:.4f}\")\n",
    "\n",
    "        # Log to wandb\n",
    "        # wandb.log({\n",
    "        #     \"epoch\": epoch + 1,\n",
    "        #     \"train_loss\": epoch_loss,\n",
    "        # })\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\n",
    "        f\"Starting training with {Config.distance_metric}, {Config.loss_type}, and {Config.backbone}...\")\n",
    "    train_model()\n",
    "    # Load best model for evaluation\n",
    "    checkpoint = torch.load(Config.checkpoint_path, weights_only=False, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # Evaluate on test set\n",
    "    evaluate(model, test_loader, Config.distance_metric, Config.top_k)\n",
    "\n",
    "    # Example image retrieval\n",
    "    query_img_path = os.path.join(\n",
    "        Config.test_dir, test_dataset.data.iloc[0]['name'])\n",
    "    results = retrieve_images(\n",
    "        model, query_img_path, test_loader, Config.top_k, Config.distance_metric)\n",
    "    visualize_retrieval(query_img_path, results, Config.test_dir)\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2e580",
   "metadata": {
    "papermill": {
     "duration": 3.957446,
     "end_time": "2025-04-23T06:29:59.585658",
     "exception": false,
     "start_time": "2025-04-23T06:29:55.628212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3114765,
     "sourceId": 5368434,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29085.007945,
   "end_time": "2025-04-23T06:30:06.474936",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-22T22:25:21.466991",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
