{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d53c0da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:26:13.509048Z",
     "iopub.status.busy": "2025-05-10T11:26:13.508786Z",
     "iopub.status.idle": "2025-05-10T11:26:13.515672Z",
     "shell.execute_reply": "2025-05-10T11:26:13.514878Z"
    },
    "papermill": {
     "duration": 0.011249,
     "end_time": "2025-05-10T11:26:13.516795",
     "exception": false,
     "start_time": "2025-05-10T11:26:13.505546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File started\n"
     ]
    }
   ],
   "source": [
    "print(\"File started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a473f658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:26:13.521203Z",
     "iopub.status.busy": "2025-05-10T11:26:13.520981Z",
     "iopub.status.idle": "2025-05-10T11:26:20.406919Z",
     "shell.execute_reply": "2025-05-10T11:26:20.406119Z"
    },
    "papermill": {
     "duration": 6.889392,
     "end_time": "2025-05-10T11:26:20.408163",
     "exception": false,
     "start_time": "2025-05-10T11:26:13.518771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\r\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\r\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\r\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\r\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.1)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpratham3992\u001b[0m (\u001b[33mpratham3992-plaksha\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"wandb_api_key\")\n",
    "\n",
    "!pip3 install wandb\n",
    "import wandb\n",
    "wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d00caa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:26:20.413297Z",
     "iopub.status.busy": "2025-05-10T11:26:20.413069Z",
     "iopub.status.idle": "2025-05-10T11:26:20.416781Z",
     "shell.execute_reply": "2025-05-10T11:26:20.416069Z"
    },
    "papermill": {
     "duration": 0.007497,
     "end_time": "2025-05-10T11:26:20.417931",
     "exception": false,
     "start_time": "2025-05-10T11:26:20.410434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb login done\n"
     ]
    }
   ],
   "source": [
    "print(\"Wandb login done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aca94424",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-10T11:26:20.422957Z",
     "iopub.status.busy": "2025-05-10T11:26:20.422774Z",
     "iopub.status.idle": "2025-05-10T14:44:04.313903Z",
     "shell.execute_reply": "2025-05-10T14:44:04.312970Z"
    },
    "papermill": {
     "duration": 11863.895514,
     "end_time": "2025-05-10T14:44:04.315389",
     "exception": false,
     "start_time": "2025-05-10T11:26:20.419875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250510_112630-kwb6ivst\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpolar-totem-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pratham3992-plaksha/visual-product-recognition\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pratham3992-plaksha/visual-product-recognition/runs/kwb6ivst\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 9691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b1-c27df63c.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b1-c27df63c.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30.1M/30.1M [00:00<00:00, 197MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7097/7097 [15:20<00:00,  7.71it/s, loss=2.6591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 5.1097 Acc: 0.2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1775/1775 [03:19<00:00,  8.90it/s, loss=0.9132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 3.0046 Acc: 0.4365\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7097/7097 [14:08<00:00,  8.37it/s, loss=3.6372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.4062 Acc: 0.5180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1775/1775 [03:05<00:00,  9.55it/s, loss=0.2114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.4139 Acc: 0.5347\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7097/7097 [14:12<00:00,  8.33it/s, loss=3.4726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.7861 Acc: 0.6178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1775/1775 [03:09<00:00,  9.38it/s, loss=0.4449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.3192 Acc: 0.5563\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7097/7097 [14:25<00:00,  8.20it/s, loss=1.8240]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.4998 Acc: 0.6686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1775/1775 [03:09<00:00,  9.37it/s, loss=0.5384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.3283 Acc: 0.5652\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7097/7097 [14:31<00:00,  8.14it/s, loss=3.3297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3318 Acc: 0.7005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1775/1775 [03:04<00:00,  9.64it/s, loss=0.7799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.3542 Acc: 0.5723\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7097/7097 [14:18<00:00,  8.26it/s, loss=1.9090]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.2078 Acc: 0.7234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1775/1775 [03:10<00:00,  9.31it/s, loss=0.0168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.3702 Acc: 0.5760\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7097/7097 [14:28<00:00,  8.17it/s, loss=2.1659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.1255 Acc: 0.7405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1775/1775 [03:07<00:00,  9.46it/s, loss=0.1685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.4364 Acc: 0.5653\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7097/7097 [14:16<00:00,  8.29it/s, loss=2.1762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0617 Acc: 0.7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1775/1775 [03:04<00:00,  9.64it/s, loss=0.9039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.4607 Acc: 0.5734\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7097/7097 [14:16<00:00,  8.29it/s, loss=0.7482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.9995 Acc: 0.7659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1775/1775 [03:04<00:00,  9.60it/s, loss=3.4225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.4957 Acc: 0.5694\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7097/7097 [14:28<00:00,  8.17it/s, loss=2.9500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.9635 Acc: 0.7742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1775/1775 [03:01<00:00,  9.78it/s, loss=0.0310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.5110 Acc: 0.5718\n",
      "\n",
      "Training complete in 175m 47s\n",
      "Best val Acc: 0.5760\n",
      "Finetuning on balanced subset...\n",
      "Starting finetuning on balanced subset...\n",
      "Epoch 1/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetuning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2863/2863 [07:12<00:00,  6.62it/s, loss=0.8624]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Loss: 1.8381 Acc: 0.6523\n",
      "Epoch 2/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetuning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2863/2863 [07:05<00:00,  6.73it/s, loss=4.7554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Loss: 1.0469 Acc: 0.7925\n",
      "Epoch 3/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetuning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2863/2863 [07:03<00:00,  6.75it/s, loss=2.2463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune Loss: 0.7329 Acc: 0.8496\n",
      "Finetuning complete in 21m 22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading product_model.pth; uploading output.log; uploading wandb-summary.json; uploading config.yaml; uploading best_model.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading product_model.pth; uploading wandb-summary.json; uploading best_model.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading product_model.pth; uploading best_model.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading product_model.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   finetune_accuracy ‚ñÅ‚ñÜ‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: finetune_batch_loss ‚ñÑ‚ñÖ‚ñá‚ñà‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      finetune_epoch ‚ñÅ‚ñÖ‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       finetune_loss ‚ñà‚ñÉ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_accuracy ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_batch_loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_accuracy ‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_batch_loss ‚ñá‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÅ‚ñá‚ñÖ‚ñÇ‚ñÉ‚ñà‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñà‚ñÇ‚ñÖ‚ñÑ‚ñÅ‚ñá‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   best_val_accuracy 0.57604\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   finetune_accuracy 0.84964\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: finetune_batch_loss 2.24632\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      finetune_epoch 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       finetune_loss 0.73292\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_accuracy 0.77421\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_batch_loss 2.94998\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.96348\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_accuracy 0.57185\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_batch_loss 0.03103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 2.51105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mpolar-totem-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pratham3992-plaksha/visual-product-recognition/runs/kwb6ivst\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pratham3992-plaksha/visual-product-recognition\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250510_112630-kwb6ivst/logs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "import wandb  # Import wandb\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None, balanced_subset=False, max_per_class=5):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        if balanced_subset:\n",
    "            self.df = self._create_balanced_subset(max_per_class)\n",
    "        \n",
    "        self.num_classes = len(self.df['class'].unique())\n",
    "        \n",
    "    def _create_balanced_subset(self, max_per_class):\n",
    "        balanced_df = pd.DataFrame()\n",
    "        \n",
    "        for class_id in self.df['class'].unique():\n",
    "            class_df = self.df[self.df['class'] == class_id]\n",
    "            if len(class_df) > max_per_class:\n",
    "                sampled_df = class_df.sample(max_per_class, random_state=42)\n",
    "                balanced_df = pd.concat([balanced_df, sampled_df])\n",
    "            else:\n",
    "                balanced_df = pd.concat([balanced_df, class_df])\n",
    "        \n",
    "        return balanced_df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name = os.path.join(self.img_dir, self.df.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        class_id = self.df.iloc[idx, 1]\n",
    "        group_id = self.df.iloc[idx, 2] if 'group' in self.df.columns else -1\n",
    "        \n",
    "        sample = {'image': image, 'class': class_id, 'group': group_id, 'filename': self.df.iloc[idx, 0]}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "        \n",
    "        return sample\n",
    "\n",
    "def get_data_transforms(high_res=False):\n",
    "    resize_size = 512 if high_res else 256\n",
    "    crop_size = 448 if high_res else 224\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((resize_size, resize_size)),\n",
    "        transforms.RandomCrop(crop_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((resize_size, resize_size)),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.3):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        pos_dist = torch.sum((anchor - positive) ** 2, dim=1)\n",
    "        neg_dist = torch.sum((anchor - negative) ** 2, dim=1)\n",
    "        loss = torch.mean(torch.clamp(pos_dist - neg_dist + self.margin, min=0))\n",
    "        return loss\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_dim=1536, pretrained=True):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        \n",
    "        model = models.efficientnet_b1(weights = 'DEFAULT' if pretrained else None)\n",
    "        \n",
    "        self.features = nn.Sequential(*list(model.children())[:-1])\n",
    "        \n",
    "        in_features = model.classifier[1].in_features\n",
    "        self.embedding = nn.Linear(in_features, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        features = features.flatten(start_dim=1)\n",
    "        \n",
    "        embeddings = self.embedding(features)\n",
    "        classifier = self.fc(embeddings)\n",
    "        \n",
    "        return embeddings, classifier\n",
    "    \n",
    "    def extract_features(self, x):\n",
    "        features = self.features(x)\n",
    "        features = features.flatten(start_dim=1)\n",
    "        embeddings = self.embedding(features)\n",
    "        return embeddings\n",
    "\n",
    "class AccuracyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AccuracyLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        if targets.dim() == 1:\n",
    "            targets = torch.nn.functional.one_hot(targets, num_classes=outputs.size(1)).float()\n",
    "        \n",
    "        correct_probs = (outputs * targets).sum(dim=1)\n",
    "        loss = 1.0 - torch.mean(correct_probs)\n",
    "        return loss\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25, use_metric_loss=False):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            pbar = tqdm(dataloaders[phase], desc=phase)\n",
    "            for batch in pbar:\n",
    "                inputs = batch['image'].to(device)\n",
    "                labels = batch['class'].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    embeddings, outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    if not use_metric_loss:\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    else:\n",
    "                        probs = nn.functional.softmax(outputs, dim=1)\n",
    "                        loss = criterion(probs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                # Update tqdm with current loss\n",
    "                pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "                \n",
    "                # Log batch-level metrics to wandb\n",
    "                wandb.log({f\"{phase}_batch_loss\": loss.item()})\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # Log epoch-level metrics to wandb\n",
    "            wandb.log({\n",
    "                f\"{phase}_loss\": epoch_loss,\n",
    "                f\"{phase}_accuracy\": epoch_acc,\n",
    "                \"epoch\": epoch\n",
    "            })\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "                # Log best model checkpoint to wandb\n",
    "                torch.save(model.state_dict(), \"best_model.pth\")\n",
    "                wandb.save(\"best_model.pth\")\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            # Log learning rate\n",
    "            wandb.log({\"learning_rate\": optimizer.param_groups[0]['lr'], \"epoch\": epoch})\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "    \n",
    "    # Log final best accuracy\n",
    "    wandb.log({\"best_val_accuracy\": best_acc})\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "def finetune_model(model, balanced_dataloader, criterion, optimizer, num_epochs=3, use_metric_loss=False):\n",
    "    print(\"Starting finetuning on balanced subset...\")\n",
    "    since = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        pbar = tqdm(balanced_dataloader, desc=\"Finetuning\")\n",
    "        for batch in pbar:\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['class'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            embeddings, outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            if not use_metric_loss:\n",
    "                loss = criterion(outputs, labels)\n",
    "            else:\n",
    "                probs = nn.functional.softmax(outputs, dim=1)\n",
    "                loss = criterion(probs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            # Update tqdm with current loss\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "            \n",
    "            # Log batch-level metrics to wandb\n",
    "            wandb.log({\"finetune_batch_loss\": loss.item()})\n",
    "        \n",
    "        epoch_loss = running_loss / len(balanced_dataloader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(balanced_dataloader.dataset)\n",
    "        \n",
    "        print(f'Finetune Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Log epoch-level metrics to wandb\n",
    "        wandb.log({\n",
    "            \"finetune_loss\": epoch_loss,\n",
    "            \"finetune_accuracy\": epoch_acc,\n",
    "            \"finetune_epoch\": epoch\n",
    "        })\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Finetuning complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Set parameters directly here instead of using argparse\n",
    "    train_dir = '/kaggle/input/visual-product-recognition/train/train'\n",
    "    train_csv = '/kaggle/input/visual-product-recognition/train.csv'\n",
    "    batch_size = 16\n",
    "    epochs = 10\n",
    "    high_res = False\n",
    "    balanced_finetune = True\n",
    "    metric_loss = False\n",
    "    lr = 3e-4\n",
    "    finetune_lr = 3e-5\n",
    "    embedding_dim = 1280\n",
    "    weight_decay = 1e-4\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"visual-product-recognition\",\n",
    "        config={\n",
    "            \"model\": \"efficientnet_b1\",\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": lr,\n",
    "            \"finetune_learning_rate\": finetune_lr,\n",
    "            \"embedding_dim\": embedding_dim,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"high_res\": high_res,\n",
    "            \"balanced_finetune\": balanced_finetune,\n",
    "            \"metric_loss\": metric_loss\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get data transformations\n",
    "    train_transform, val_transform = get_data_transforms(high_res=high_res)\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = ProductDataset(train_csv, train_dir, transform=train_transform)\n",
    "    \n",
    "    # Create a validation split from the training data\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Update validation subset transform\n",
    "    val_subset.dataset.transform = val_transform\n",
    "    \n",
    "    # Create data loaders for training and validation\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "    \n",
    "    # Create balanced dataset for finetuning if specified\n",
    "    if balanced_finetune:\n",
    "        balanced_dataset = ProductDataset(\n",
    "            train_csv, train_dir, transform=train_transform, \n",
    "            balanced_subset=True, max_per_class=5\n",
    "        )\n",
    "        balanced_loader = DataLoader(balanced_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # Get the number of classes from the dataset\n",
    "    num_classes = train_dataset.num_classes\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    wandb.config.update({\"num_classes\": num_classes})\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = FeatureExtractor(num_classes=num_classes, embedding_dim=embedding_dim)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Log model architecture to wandb\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # If using metric-guided loss, use our custom AccuracyLoss\n",
    "    if metric_loss:\n",
    "        metric_criterion = AccuracyLoss()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 40, 60], gamma=0.1)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting model training...\")\n",
    "    model = train_model(\n",
    "        model, dataloaders, criterion, optimizer, scheduler,\n",
    "        num_epochs=epochs, use_metric_loss=False\n",
    "    )\n",
    "    \n",
    "    # Finetune on balanced subset if specified\n",
    "    if balanced_finetune:\n",
    "        print(\"Finetuning on balanced subset...\")\n",
    "        finetune_optimizer = optim.Adam(model.parameters(), lr=finetune_lr, weight_decay=weight_decay)\n",
    "        \n",
    "        if metric_loss:\n",
    "            model = finetune_model(\n",
    "                model, balanced_loader, metric_criterion, finetune_optimizer, \n",
    "                num_epochs=1, use_metric_loss=True\n",
    "            )\n",
    "        else:\n",
    "            model = finetune_model(\n",
    "                model, balanced_loader, criterion, finetune_optimizer, \n",
    "                num_epochs=3, use_metric_loss=False\n",
    "            )\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'product_model.pth')\n",
    "    wandb.save('product_model.pth')\n",
    "    \n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "    \n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3114765,
     "sourceId": 5368434,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11885.533485,
   "end_time": "2025-05-10T14:44:14.859321",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-10T11:26:09.325836",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
