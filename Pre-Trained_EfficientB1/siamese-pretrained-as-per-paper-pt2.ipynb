{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e4a4f18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T21:36:01.414785Z",
     "iopub.status.busy": "2025-05-10T21:36:01.414156Z",
     "iopub.status.idle": "2025-05-10T21:36:01.420746Z",
     "shell.execute_reply": "2025-05-10T21:36:01.420046Z"
    },
    "papermill": {
     "duration": 0.010656,
     "end_time": "2025-05-10T21:36:01.421840",
     "exception": false,
     "start_time": "2025-05-10T21:36:01.411184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File started\n"
     ]
    }
   ],
   "source": [
    "print(\"File started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e34bac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T21:36:01.425968Z",
     "iopub.status.busy": "2025-05-10T21:36:01.425801Z",
     "iopub.status.idle": "2025-05-10T21:36:04.491787Z",
     "shell.execute_reply": "2025-05-10T21:36:04.491018Z"
    },
    "papermill": {
     "duration": 3.069127,
     "end_time": "2025-05-10T21:36:04.492857",
     "exception": false,
     "start_time": "2025-05-10T21:36:01.423730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpratham3992\u001b[0m (\u001b[33mpratham3992-plaksha\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"wandb_api_key\")\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acfbd7e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T21:36:04.497742Z",
     "iopub.status.busy": "2025-05-10T21:36:04.497311Z",
     "iopub.status.idle": "2025-05-10T21:36:04.501009Z",
     "shell.execute_reply": "2025-05-10T21:36:04.500382Z"
    },
    "papermill": {
     "duration": 0.007184,
     "end_time": "2025-05-10T21:36:04.502073",
     "exception": false,
     "start_time": "2025-05-10T21:36:04.494889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb login done\n"
     ]
    }
   ],
   "source": [
    "print(\"Wandb login done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c87ad200",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-10T21:36:04.507149Z",
     "iopub.status.busy": "2025-05-10T21:36:04.506807Z",
     "iopub.status.idle": "2025-05-10T22:29:15.738970Z",
     "shell.execute_reply": "2025-05-10T22:29:15.738267Z"
    },
    "papermill": {
     "duration": 3191.236325,
     "end_time": "2025-05-10T22:29:15.740340",
     "exception": false,
     "start_time": "2025-05-10T21:36:04.504015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250510_213616-exsmq3nl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mimage-retrieval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/pratham3992-plaksha/visual-product-recognition\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/pratham3992-plaksha/visual-product-recognition/runs/exsmq3nl\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 9691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b1-c27df63c.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b1-c27df63c.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30.1M/30.1M [00:00<00:00, 132MB/s]\n",
      "/tmp/ipykernel_18/2715348930.py:461: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('/kaggle/input/metricloss-false/pytorch/default/1/product_model.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pre-trained product model weights\n",
      "Extracting gallery features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8871/8871 [17:13<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting query features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3461/3461 [05:32<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete.\n",
      "Average Precision@5 (cosine distance): 0.4849\n",
      "Average Precision@5 (euclidean distance): 0.0000\n",
      "Average Precision@5 (dot distance): 0.4008\n",
      "mAP@5 (cosine distance): 0.6363\n",
      "mAP@5 (euclidean distance): 0.0000\n",
      "mAP@5 (dot distance): 0.5486\n",
      "\n",
      "Comparison of distance metrics (Precision@5 and mAP@5):\n",
      "==================================================\n",
      "Distance Metric Precision@5 mAP@5     \n",
      "--------------------------------------------------\n",
      "cosine          0.4849     0.6363    \n",
      "euclidean       0.0000     0.0000    \n",
      "dot             0.4008     0.5486    \n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading artifact run-exsmq3nl-precision_map_comparison_at_5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          map@5_cosine ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             map@5_dot ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       map@5_euclidean ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    precision@5_cosine ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       precision@5_dot ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: precision@5_euclidean ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          map@5_cosine 0.63631\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             map@5_dot 0.54863\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       map@5_euclidean 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    precision@5_cosine 0.48493\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       precision@5_dot 0.40081\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: precision@5_euclidean 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mimage-retrieval\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/pratham3992-plaksha/visual-product-recognition/runs/exsmq3nl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/pratham3992-plaksha/visual-product-recognition\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 11 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250510_213616-exsmq3nl/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import wandb  # Import wandb\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Global variables for image size - will be set in main()\n",
    "img_size = 224  # default for standard resolution\n",
    "resize_size = 256  # default for standard resolution\n",
    "\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        if 'group' not in self.df.columns:\n",
    "            self.df['group'] = -1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name = os.path.join(self.img_dir, self.df.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        class_id = self.df.iloc[idx, 1]\n",
    "        group_id = self.df.iloc[idx, 2] if 'group' in self.df.columns else -1\n",
    "        \n",
    "        sample = {'image': image, 'class': class_id, 'group': group_id, 'filename': self.df.iloc[idx, 0]}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "        \n",
    "        return sample\n",
    "\n",
    "def get_data_transforms(high_res=False):\n",
    "    resize_size = 512 if high_res else 256\n",
    "    crop_size = 448 if high_res else 224\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((resize_size, resize_size)),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        \n",
    "        model = models.efficientnet_b1(weights='DEFAULT' if pretrained else None)\n",
    "        \n",
    "        self.features = nn.Sequential(*list(model.children())[:-1])\n",
    "        \n",
    "        in_features = model.classifier[1].in_features\n",
    "        self.fc = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        features = features.flatten(start_dim=1)\n",
    "        \n",
    "        classifier = self.fc(features)\n",
    "        \n",
    "        return features, classifier\n",
    "    \n",
    "    def extract_features(self, x):\n",
    "        features = self.features(x)\n",
    "        features = features.flatten(start_dim=1)\n",
    "        return features\n",
    "\n",
    "def compute_similarity(query_features, gallery_features, method='cosine'):\n",
    "    # Convert to PyTorch tensors if they're numpy arrays\n",
    "    if isinstance(query_features, np.ndarray):\n",
    "        query_features = torch.from_numpy(query_features).to(device)\n",
    "    if isinstance(gallery_features, np.ndarray):\n",
    "        gallery_features = torch.from_numpy(gallery_features).to(device)\n",
    "    \n",
    "    if method == 'cosine':\n",
    "        query_norm = query_features / torch.norm(query_features, dim=1, keepdim=True)\n",
    "        gallery_norm = gallery_features / torch.norm(gallery_features, dim=1, keepdim=True)\n",
    "        similarity = torch.mm(query_norm, gallery_norm.T)\n",
    "    elif method == 'euclidean':\n",
    "        # Compute pairwise euclidean distance using PyTorch\n",
    "        similarity = -torch.cdist(query_features, gallery_features, p=2.0)\n",
    "    elif method == 'dot':\n",
    "        similarity = torch.mm(query_features, gallery_features.T)\n",
    "    \n",
    "    # Return as numpy for compatibility with the rest of the code\n",
    "    return similarity.cpu().numpy()\n",
    "\n",
    "def extract_all_features(model, dataloader):\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_classes = []\n",
    "    all_groups = []\n",
    "    all_filenames = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            images = batch['image'].to(device)\n",
    "            classes = batch['class']\n",
    "            groups = batch['group']\n",
    "            filenames = batch['filename']\n",
    "            \n",
    "            features = model.extract_features(images)\n",
    "            \n",
    "            # Keep the features on CPU as NumPy arrays for consistency with the rest of the code\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_classes.extend(classes.numpy())\n",
    "            all_groups.extend(groups.numpy())\n",
    "            all_filenames.extend(filenames)\n",
    "    \n",
    "    all_features = np.concatenate(all_features, axis=0)\n",
    "    all_classes = np.array(all_classes)\n",
    "    all_groups = np.array(all_groups)\n",
    "    \n",
    "    return all_features, all_classes, all_groups, all_filenames\n",
    "\n",
    "def visualize_retrieval_results(query_img_dir, gallery_img_dir, query_filename, retrieval_filenames, \n",
    "                                query_class, retrieval_classes, query_group, retrieval_groups, \n",
    "                                similarity_scores, k=5):\n",
    "    \"\"\"Visualize image retrieval results\"\"\"\n",
    "    os.makedirs('retrieval_results', exist_ok=True)\n",
    "    \n",
    "    query_img_path = os.path.join(query_img_dir, query_filename)\n",
    "    query_img = Image.open(query_img_path).convert('RGB')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, k + 1, figsize=(15, 3))\n",
    "    \n",
    "    axes[0].imshow(query_img)\n",
    "    axes[0].set_title(f'Query Image\\nClass: {query_class}, Group: {query_group}')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    for i in range(k):\n",
    "        if i < len(retrieval_filenames):\n",
    "            gallery_img_path = os.path.join(gallery_img_dir, retrieval_filenames[i])\n",
    "            gallery_img = Image.open(gallery_img_path).convert('RGB')\n",
    "            \n",
    "            axes[i+1].imshow(gallery_img)\n",
    "            match_text = \"Same Class\" if retrieval_classes[i] == query_class else \"Different Class\"\n",
    "            group_text = f\"Group: {retrieval_groups[i]}\"\n",
    "            axes[i+1].set_title(f'Rank {i+1}: {match_text}\\n{group_text}\\nSimilarity: {similarity_scores[i]:.4f}')\n",
    "            axes[i+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    result_filepath = f'retrieval_results/{os.path.splitext(query_filename)[0]}_retrieval.png'\n",
    "    plt.savefig(result_filepath)\n",
    "    plt.close()\n",
    "    \n",
    "    # Log the visualization to wandb\n",
    "    wandb.log({\n",
    "        f\"retrieval_{os.path.splitext(query_filename)[0]}\": wandb.Image(\n",
    "            result_filepath,\n",
    "            caption=f\"Query: {query_filename}, Class: {query_class}, Group: {query_group}\"\n",
    "        )\n",
    "    })\n",
    "\n",
    "def perform_retrieval_visualization(query_features, query_classes, query_groups, query_filenames,\n",
    "                               gallery_features, gallery_classes, gallery_groups, gallery_filenames,\n",
    "                               distance_method, query_img_dir, gallery_img_dir, top_k=5, num_queries=10):\n",
    "    \"\"\"Visualizes retrieval results for selected queries\"\"\"\n",
    "    # Ensure we don't try to retrieve more queries than exist\n",
    "    num_queries = min(num_queries, len(query_features))\n",
    "    \n",
    "    # Convert features to tensors if they're not already\n",
    "    if isinstance(query_features, np.ndarray):\n",
    "        query_features = torch.from_numpy(query_features).to(device)\n",
    "    if isinstance(gallery_features, np.ndarray):\n",
    "        gallery_features = torch.from_numpy(gallery_features).to(device)\n",
    "    \n",
    "    # Randomly select subset of queries for visualization\n",
    "    query_indices = np.random.choice(len(query_features), num_queries, replace=False)\n",
    "    \n",
    "    for idx in query_indices:\n",
    "        query_feature = query_features[idx:idx+1]\n",
    "        query_class = query_classes[idx]\n",
    "        query_group = query_groups[idx]\n",
    "        query_filename = query_filenames[idx]\n",
    "        \n",
    "        # Compute similarity between query and all gallery images\n",
    "        similarity = compute_similarity(query_feature, gallery_features, method=distance_method)\n",
    "        similarity = similarity[0]  # Take the first row as we only have one query\n",
    "        \n",
    "        # Get top k indices\n",
    "        if distance_method in ['cosine', 'dot']:\n",
    "            top_indices = np.argsort(similarity)[::-1][:top_k]\n",
    "        else:  # euclidean - smaller distance is better\n",
    "            top_indices = np.argsort(similarity)[:top_k]\n",
    "            \n",
    "        retrieval_filenames = [gallery_filenames[i] for i in top_indices]\n",
    "        retrieval_classes = [gallery_classes[i] for i in top_indices]\n",
    "        retrieval_groups = [gallery_groups[i] for i in top_indices]\n",
    "        retrieval_scores = [similarity[i] for i in top_indices]\n",
    "        \n",
    "        # Visualize the retrieval results\n",
    "        visualize_retrieval_results(\n",
    "            query_img_dir, gallery_img_dir, query_filename, retrieval_filenames,\n",
    "            query_class, retrieval_classes, query_group, retrieval_groups,\n",
    "            retrieval_scores, k=top_k\n",
    "        )\n",
    "\n",
    "def calculate_precision(query_features, query_classes, gallery_features, gallery_classes, method='cosine', top_k=5):\n",
    "    \"\"\"Calculate precision@k for a given distance method\"\"\"\n",
    "    # Convert inputs to tensors on device if they're not already\n",
    "    if isinstance(query_features, np.ndarray):\n",
    "        query_features = torch.from_numpy(query_features).to(device)\n",
    "    if isinstance(gallery_features, np.ndarray):\n",
    "        gallery_features = torch.from_numpy(gallery_features).to(device)\n",
    "    \n",
    "    query_classes_np = query_classes  # Keep a numpy version for indexing\n",
    "    if isinstance(gallery_classes, np.ndarray):\n",
    "        gallery_classes_tensor = torch.from_numpy(gallery_classes).to(device)\n",
    "    else:\n",
    "        gallery_classes_tensor = gallery_classes\n",
    "    \n",
    "    batch_size = 100  # Process queries in batches to avoid memory issues\n",
    "    num_queries = len(query_features)\n",
    "    all_precision = []\n",
    "    \n",
    "    for batch_start in range(0, num_queries, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, num_queries)\n",
    "        batch_query_features = query_features[batch_start:batch_end]\n",
    "        \n",
    "        # Compute similarity for the entire batch\n",
    "        similarity_matrix = compute_similarity(batch_query_features, gallery_features, method=method)\n",
    "        \n",
    "        # Process each query in the batch\n",
    "        for i in range(batch_end - batch_start):\n",
    "            idx = batch_start + i\n",
    "            query_class = query_classes_np[idx]\n",
    "            similarity = similarity_matrix[i]\n",
    "            \n",
    "            if method in ['cosine', 'dot']:\n",
    "                top_indices = np.argsort(similarity)[::-1][:top_k]\n",
    "            else:\n",
    "                top_indices = np.argsort(similarity)[:top_k]\n",
    "            \n",
    "            retrieval_classes = [gallery_classes[j] for j in top_indices]\n",
    "            \n",
    "            # Calculate precision\n",
    "            correct = sum([1 for c in retrieval_classes if c == query_class])\n",
    "            precision = correct / top_k\n",
    "            all_precision.append(precision)\n",
    "    \n",
    "    avg_precision = np.mean(all_precision)\n",
    "    print(f\"Average Precision@{top_k} ({method} distance): {avg_precision:.4f}\")\n",
    "    \n",
    "    # Log precision metric to wandb\n",
    "    wandb.log({f\"precision@{top_k}_{method}\": avg_precision})\n",
    "    \n",
    "    return avg_precision\n",
    "\n",
    "def calculate_ap_at_k(relevant_scores, k=5):\n",
    "    \"\"\"\n",
    "    Calculate Average Precision @ K.\n",
    "    \n",
    "    Args:\n",
    "        relevant_scores: Binary list indicating if each retrieval is relevant (same class)\n",
    "        k: Cut-off rank (consider only top k retrievals)\n",
    "    \n",
    "    Returns:\n",
    "        AP@k score\n",
    "    \"\"\"\n",
    "    relevant_scores = relevant_scores[:k]  # Consider only top k\n",
    "    \n",
    "    if not any(relevant_scores):  # No relevant retrievals\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate precision at each relevant position\n",
    "    precisions = []\n",
    "    num_relevant = 0\n",
    "    \n",
    "    for i, is_relevant in enumerate(relevant_scores):\n",
    "        if is_relevant:\n",
    "            num_relevant += 1\n",
    "            # Precision at position i+1 (0-indexed to 1-indexed)\n",
    "            precision_at_i = num_relevant / (i + 1)\n",
    "            precisions.append(precision_at_i)\n",
    "    \n",
    "    # AP is the average of precisions at relevant positions\n",
    "    ap = sum(precisions) / min(sum(relevant_scores), k)\n",
    "    return ap\n",
    "\n",
    "def calculate_map(query_features, query_classes, gallery_features, gallery_classes, method='cosine', top_k=5):\n",
    "    \"\"\"Calculate mAP@k for a given distance method\"\"\"\n",
    "    # Convert inputs to tensors on device if they're not already\n",
    "    if isinstance(query_features, np.ndarray):\n",
    "        query_features = torch.from_numpy(query_features).to(device)\n",
    "    if isinstance(gallery_features, np.ndarray):\n",
    "        gallery_features = torch.from_numpy(gallery_features).to(device)\n",
    "    \n",
    "    query_classes_np = query_classes if isinstance(query_classes, np.ndarray) else query_classes.cpu().numpy()\n",
    "    gallery_classes_np = gallery_classes if isinstance(gallery_classes, np.ndarray) else gallery_classes.cpu().numpy()\n",
    "    \n",
    "    batch_size = 100  # Process queries in batches to avoid memory issues\n",
    "    num_queries = len(query_features)\n",
    "    all_ap = []\n",
    "    \n",
    "    for batch_start in range(0, num_queries, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, num_queries)\n",
    "        batch_query_features = query_features[batch_start:batch_end]\n",
    "        \n",
    "        # Compute similarity for the entire batch\n",
    "        similarity_matrix = compute_similarity(batch_query_features, gallery_features, method=method)\n",
    "        \n",
    "        # Process each query in the batch\n",
    "        for i in range(batch_end - batch_start):\n",
    "            idx = batch_start + i\n",
    "            query_class = query_classes_np[idx]\n",
    "            similarity = similarity_matrix[i]\n",
    "            \n",
    "            # Get ranking indices\n",
    "            if method in ['cosine', 'dot']:\n",
    "                top_indices = np.argsort(similarity)[::-1][:top_k]\n",
    "            else:\n",
    "                top_indices = np.argsort(similarity)[:top_k]\n",
    "                \n",
    "            # Get relevance scores (1 for same class, 0 for different)\n",
    "            relevant_scores = [1 if gallery_classes_np[j] == query_class else 0 for j in top_indices]\n",
    "            \n",
    "            # Calculate AP@k for this query\n",
    "            ap = calculate_ap_at_k(relevant_scores, top_k)\n",
    "            all_ap.append(ap)\n",
    "    \n",
    "    # Calculate mAP@k (mean of APs)\n",
    "    map_k = np.mean(all_ap)\n",
    "    print(f\"mAP@{top_k} ({method} distance): {map_k:.4f}\")\n",
    "    \n",
    "    # Log mAP metric to wandb\n",
    "    wandb.log({f\"map@{top_k}_{method}\": map_k})\n",
    "    \n",
    "    return map_k\n",
    "\n",
    "def perform_retrieval(model, query_dataloader, gallery_dataloader, distance_method, query_img_dir, gallery_img_dir, top_k=5, num_queries=10, \n",
    "                  pre_extracted_features=None):\n",
    "    # Use pre-extracted features if provided, otherwise extract them\n",
    "    if pre_extracted_features:\n",
    "        query_features, query_classes, query_groups, query_filenames, gallery_features, gallery_classes, gallery_groups, gallery_filenames = pre_extracted_features\n",
    "    else:\n",
    "        # Extract features for gallery and query images\n",
    "        gallery_features, gallery_classes, gallery_groups, gallery_filenames = extract_all_features(model, gallery_dataloader)\n",
    "        query_features, query_classes, query_groups, query_filenames = extract_all_features(model, query_dataloader)\n",
    "    \n",
    "    # Do visualization with the primary distance method\n",
    "    perform_retrieval_visualization(\n",
    "        query_features, query_classes, query_groups, query_filenames,\n",
    "        gallery_features, gallery_classes, gallery_groups, gallery_filenames,\n",
    "        distance_method=distance_method, \n",
    "        top_k=top_k,\n",
    "        query_img_dir=query_img_dir,\n",
    "        gallery_img_dir=gallery_img_dir,\n",
    "        num_queries=num_queries\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics for all distance methods\n",
    "    all_distances = ['cosine', 'euclidean', 'dot']\n",
    "    results = {}\n",
    "    \n",
    "    for dist_method in all_distances:\n",
    "        avg_precision = calculate_precision(\n",
    "            query_features, query_classes,\n",
    "            gallery_features, gallery_classes,\n",
    "            method=dist_method,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        results[dist_method] = avg_precision\n",
    "    \n",
    "    # Calculate mAP for all distance methods\n",
    "    map_results = {}\n",
    "    for dist_method in all_distances:\n",
    "        map_k = calculate_map(\n",
    "            query_features, query_classes,\n",
    "            gallery_features, gallery_classes,\n",
    "            method=dist_method,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        map_results[dist_method] = map_k\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(\"\\nComparison of distance metrics (Precision@{} and mAP@{}):\".format(top_k, top_k))\n",
    "    print(\"=\" * 50)\n",
    "    print(\"{:<15} {:<10} {:<10}\".format(\"Distance Metric\", f\"Precision@{top_k}\", f\"mAP@{top_k}\"))\n",
    "    print(\"-\" * 50)\n",
    "    for dist_method in all_distances:\n",
    "        print(\"{:<15} {:<10.4f} {:<10.4f}\".format(dist_method, results[dist_method], map_results[dist_method]))\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Log comparison table to wandb\n",
    "    wandb.log({f\"precision_map_comparison_at_{top_k}\": wandb.Table(\n",
    "        columns=[\"Distance Metric\", f\"Precision@{top_k}\", f\"mAP@{top_k}\"],\n",
    "        data=[[method, results[method], map_results[method]] for method in all_distances]\n",
    "    )})\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Parameters that were previously handled by argparse\n",
    "    train_dir = '/kaggle/input/visual-product-recognition/train/train'\n",
    "    test_dir = '/kaggle/input/visual-product-recognition/test/test'\n",
    "    train_csv = '/kaggle/input/visual-product-recognition/train.csv'\n",
    "    test_csv = '/kaggle/input/visual-product-recognition/test.csv'\n",
    "    batch_size = 16\n",
    "    high_res = False  # Set to True to use high resolution images\n",
    "    top_k = 5\n",
    "    distance = 'cosine'  # Primary distance for visualization: 'cosine', 'euclidean', 'dot'\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"visual-product-recognition\",\n",
    "        name=\"image-retrieval\",\n",
    "        config={\n",
    "            \"model\": \"efficientnet_b1\",\n",
    "            \"batch_size\": batch_size,\n",
    "            \"high_res\": high_res,\n",
    "            \"top_k\": top_k,\n",
    "            \"primary_distance\": distance\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Update global variables based on high_res setting\n",
    "    global img_size, resize_size\n",
    "    img_size = 448 if high_res else 224\n",
    "    resize_size = 512 if high_res else 256\n",
    "    \n",
    "    # Get data transformations\n",
    "    transform = get_data_transforms(high_res=high_res)\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = ProductDataset(train_csv, train_dir, transform=transform)\n",
    "    test_dataset = ProductDataset(test_csv, test_dir, transform=transform)\n",
    "    \n",
    "    # Calculate number of classes\n",
    "    num_classes = len(pd.read_csv(train_csv)['class'].unique())\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    gallery_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    query_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = FeatureExtractor(num_classes=num_classes)\n",
    "    \n",
    "    # Load pre-trained weights if available, otherwise just use ImageNet weights\n",
    "    try:\n",
    "        checkpoint = torch.load('/kaggle/input/metricloss-false/pytorch/default/1/product_model.pth', map_location=device)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint, strict=False)\n",
    "        print(\"Loaded pre-trained product model weights\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load pre-trained weights: {e}\")\n",
    "        print(\"Using ImageNet pre-trained weights\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Extract features once (to avoid re-computing them for each distance metric)\n",
    "    print(\"Extracting gallery features...\")\n",
    "    gallery_features, gallery_classes, gallery_groups, gallery_filenames = extract_all_features(model, gallery_loader)\n",
    "    print(\"Extracting query features...\")\n",
    "    query_features, query_classes, query_groups, query_filenames = extract_all_features(model, query_loader)\n",
    "    print(\"Feature extraction complete.\")\n",
    "\n",
    "    # Package extracted features\n",
    "    pre_extracted_features = (\n",
    "        query_features, query_classes, query_groups, query_filenames,\n",
    "        gallery_features, gallery_classes, gallery_groups, gallery_filenames\n",
    "    )\n",
    "\n",
    "    # Perform retrieval and visualization with pre-extracted features\n",
    "    perform_retrieval(\n",
    "        model,\n",
    "        query_loader,\n",
    "        gallery_loader,\n",
    "        distance_method=distance,\n",
    "        query_img_dir=test_dir,\n",
    "        gallery_img_dir=train_dir,\n",
    "        top_k=top_k,\n",
    "        num_queries=10,\n",
    "        pre_extracted_features=pre_extracted_features\n",
    "    )\n",
    "    \n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3114765,
     "sourceId": 5368434,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 338651,
     "modelInstanceId": 318090,
     "sourceId": 385604,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3202.460172,
   "end_time": "2025-05-10T22:29:19.382860",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-10T21:35:56.922688",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
